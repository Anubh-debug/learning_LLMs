{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1aOHNICgxXu5Ltys-30i0Y4t7XVbIcG1T",
      "authorship_tag": "ABX9TyOk11wm4mbdxNAsWPMYGC0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anubh-debug/learning_LLMs/blob/gradio_apps/constitution_gradio_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IzBsMRKZOZlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q datasets\n",
        "!pip install -q PyPDF2\n",
        "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
        "%pip install -U -q langchain-openai\n",
        "%pip install -q chromadb"
      ],
      "metadata": {
        "id": "HeA9JPoXgEo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from PyPDF2 import PdfReader\n",
        "# PdfReader converts pdf document into text.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain.chat_models import init_chat_model\n",
        "from google.colab import userdata\n",
        "import getpass\n",
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "QwUgf3lcgtrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "download indian constitution from here: https://legislative.gov.in/constitution-of-india/ and save it on your google drive"
      ],
      "metadata": {
        "id": "5UExCClIuH2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the file"
      ],
      "metadata": {
        "id": "MEO9-fbegv9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages = []\n",
        "reader = PdfReader(\"/content/drive/MyDrive/indian_constitution_eng.pdf\")\n",
        "number_of_pages = len(reader.pages)\n",
        "\n",
        "for i in range(number_of_pages):\n",
        "  page = reader.pages[i]\n",
        "  text = page.extract_text()\n",
        "  pages.append(text)\n",
        "\n",
        "# appending all the pages into a single page\n",
        "one_page=''\n",
        "for page in pages:\n",
        "  one_page += ' ' + page"
      ],
      "metadata": {
        "id": "hcZmcnFsglO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting text with textsplitter"
      ],
      "metadata": {
        "id": "zUltv06bzj0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)\n",
        "texts = text_splitter.split_text(one_page)\n",
        "print(f\"Number of chunks: {len(texts)}\")"
      ],
      "metadata": {
        "id": "sFCE6GCnuqUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize CHAT model"
      ],
      "metadata": {
        "id": "fTthAzpK2iAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "open_ai_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "EPE84qgD2heA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialize embedding model"
      ],
      "metadata": {
        "id": "GTuF6rY035gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "embedded_chunks = embeddings.embed_documents(texts)"
      ],
      "metadata": {
        "id": "qbgGchSLzi3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing chroma database"
      ],
      "metadata": {
        "id": "Y7W94BdfM7ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db_name = \"chroma_IN_constitution\"\n",
        "if os.path.exists(db_name):\n",
        "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()"
      ],
      "metadata": {
        "id": "8AlVl69RHMOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of strings to a list of Document objects\n",
        "documents = [Document(page_content=text) for text in texts]\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=db_name)\n",
        "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
      ],
      "metadata": {
        "id": "kmwfX1w1Ki2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one vector and find how many dimensions it has\n",
        "\n",
        "collection = vectorstore._collection\n",
        "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
        "dimensions = len(sample_embedding)\n",
        "print(f\"The vectors have {dimensions:,} dimensions\")"
      ],
      "metadata": {
        "id": "8yFWfncfLXx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the conversation memory for the chat\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "# the retriever is an abstraction over the VectorStore that will be used during RAG. We will retrieve top 5 relevant chunks\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "SYSTEM_MESSAGE = \"You are an Indian constitution expert.\"\n",
        "\n",
        "# Corrected prompt template structure for ConversationalRetrievalChain\n",
        "# Added {context} placeholder for retrieved documents\n",
        "custom_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", SYSTEM_MESSAGE + \"\\n\\nHere is some relevant context:\\n{context}\"),\n",
        "    (\"human\", \"{chat_history}\\n{question}\"),\n",
        "])\n",
        "\n",
        "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "J9TeT00DLoPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing our chain\n",
        "# query = \"Tell me about president powers\"\n",
        "# result = conversation_chain.invoke({\"question\":query})\n",
        "# print(result[\"answer\"])"
      ],
      "metadata": {
        "id": "kUc0A0OQMBHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat(message, history):\n",
        "    result = conversation_chain.invoke({\"question\": message})\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
      ],
      "metadata": {
        "id": "QULMi2geOqNg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}